head(sentiment)
head(sentence)
summary(sentiment)
plot(sentiment,type = "l",main = "plot Trajectory",
xlab = "Narrative Time",ylab = "Emotion Valence")
abline(h=0,col="Green")
###extract positive and negative emotion
neg <- sentence[which.min(sentiment)]
neg
pos <- sentence[which.max(sentiment)]
pos
### in  more depth
poa_v <- myfile
poa_sent <- get_sentiment(poa_v, method = "bing")
plot(poa_sent,type = "h", main = "LOTR using transformed Values",
xlab = "Narrative Time", ylab = "Emotinal Valence",col="red")
ft_values <- get_transformed_values(poa_sent,
low_pass_size = 3,
x_reverse_len = 100,
scale_vals = TRUE,
scale_range = FALSE)
plot(ft_values, type = "h", main = "LOTR using Transformed values",
xlab = "Narrative time", ylab = "Emotional Valence",
col="red")
nrc_data <- get_nrc_sentiment(sentence)
nrc_score_sent <- get_nrc_sentiment(negative)
nrc_score_word <- get_nrc_sentiment('grim')
#subset
sad_items <- which(nrc_data$sadness>0)
head(sentence[sad_items])
barplot(sort(colSums(prop.table(nrc_data[,1:8]))), horiz = T, cex.names = 0.7,
las= 1, main = "word concurrency", xlab="Percentage",col = 1:8)
######  trust is maximum in the review
## disgust is min or least according to review
## we can say that maximum number of coustmer are satisfied with product
## so we can say that product is good
library(rvest)
library(magrittr)
library(readr)
library(tm)
library(wordcloud)
library(SnowballC)
library(ggplot2)
library(twitteR)
library(ROAuth)
library(base64enc)
library(httpuv)
#install.packages(c('ROAuth','RCurl'))
require('ROAuth')
require('RCurl')
cred <- OAuthFactory$new(consumerKey='2vBpHneUggqA3g6UJLRbdfC4B', # Consumer Key (API Key)
consumerSecret='RCliA6thGAa8J91i4lNL0zMyKSXcFwcSXWbFKtjOZz8RySIBLJ', #Consumer Secret (API Secret)
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
#cred$handshake(cainfo="cacert.pem")
setwd("C://Users//Sanchi//Desktop//Assignment//Complete//R//NLP")
getwd()
save(cred, file="twitter authentication.Rdata")
load("twitter authentication.Rdata")
#install.packages("twitteR", "RCurl", "RJSONIO", "stringr")
setup_twitter_oauth("2vBpHneUggqA3g6UJLRbdfC4B", # Consumer Key (API Key)
"RCliA6thGAa8J91i4lNL0zMyKSXcFwcSXWbFKtjOZz8RySIBLJ", #Consumer Secret (API Secret)
"891539595652976640-rHXrnL2L2Tql69LQVXZipjAp7IV1s18",  # Access Token
"50SbSHaZeQoclnvixDZJgF3nCZyPZlL8YgYz9kOLNUZ7h")  #Access Token Secret
#registerTwitterOAuth(cred)
Tweets <- userTimeline('OfficialKalam', n = 1000, includeRts = T)
Tweets
TweetsDF <- twListToDF(Tweets)
twee
dim(TweetsDF)
View(TweetsDF)
write.csv(TweetsDF, "Tweets123.csv",row.names = F)
twt <- read.csv("Tweets123.csv")
twt.txt <- as.character(twt$text)
View(twt.txt)
## creating corpus data
data <- Corpus(VectorSource(twt.txt))
clean_data <- tm_map(data, tolower)
clean_data <- tm_map(clean_data, removeNumbers)
clean_data <- tm_map(clean_data, removePunctuation)
clean_data <- tm_map(clean_data, removeWords, stopwords())
clean_data <- tm_map(clean_data, removeWords, c("rt","dicapriofdn","https...","..."))
clean_data <- tm_map(clean_data, removeWords, c("rt","..."))
clean_data <- tm_map(clean_data, stripWhitespace)
clean_data <- tm_map(clean_data, stemDocument)
inspect(clean_data[1:10])
## creating Term Document Matrix
data_tdm <- TermDocumentMatrix(clean_data)
inspect(data_tdm[1:10,1:10])
a <- NULL
for (i in 1:ncol(data_tdm)){
if (sum(data_tdm[, i]) == 0){
a <-  c(a,i)
} }
## creating word cloud
m <- as.matrix(data_tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d <- d[-1,]  ## removing "..."
head(d, 10)
windows()
wordcloud(words = d$word, freq = d$freq, min.freq = 5,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# lOADING +VE AND -VE words
pos_words <- scan("C://Users//Sanchi//Desktop//Assignment//Complete//R//NLP//positive-words.txt", what="character", comment.char=";")
neg_words <- scan("C://Users//Sanchi//Desktop//Assignment//Complete//R//NLP//negative-words.txt", what="character", comment.char=";")
#pos.words=c(pos.words,"wow", "kudos", "hurray") 			# including our own positive words to the existing list
#neg.words = c(neg.words)
############### SENTIMENT ANALYSIS ##################
## Positive words bar plot
poswordbar = function(x){
freq = sort(rowSums(as.matrix(x)),decreasing = TRUE)
# matching positive words
pos.matches = match(names(freq), pos_words)
pos.matches = !is.na(pos.matches)
freq_pos <- freq[pos.matches]
names <- names(freq_pos)
df <- data.frame(names,freq_pos)
windows()
ggplot(head(df,30), aes(reorder(names,freq_pos), freq_pos)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Positive words") + ylab("Frequency") +
ggtitle("Most frequent positive words")
}
poswordbar(data_tdm)
## Positive word cloud
poswordc = function(x){
freq = sort(rowSums(as.matrix(x)),decreasing = TRUE)
# matching positive words
pos.matches = match(names(freq), pos_words)
pos.matches = !is.na(pos.matches)
freq_pos <- freq[pos.matches]
names <- names(freq_pos)
windows()
wordcloud(names,freq_pos,sacle= c(4,0.5),min.freq = 1, colors = brewer.pal(8,"Dark2"))
}
poswordc(data_tdm)
## Negative word bar plot
negwordbar = function(x){
freq = sort(rowSums(as.matrix(x)),decreasing = TRUE)
# matching positive words
neg.matches = match(names(freq), neg_words)
neg.matches = !is.na(neg.matches)
freq_neg <- freq[neg.matches]
names <- names(freq_neg)
df <- data.frame(names,freq_neg)
windows()
ggplot(head(df,30), aes(reorder(names,freq_neg), freq_neg)) +
geom_bar(stat = "identity") + coord_flip() +
xlab("Negative words") + ylab("Frequency") +
ggtitle("Most frequent negative words")
}
negwordbar(data_tdm)
## Negative word cloud
negwordc = function(x){
freq = sort(rowSums(as.matrix(x)),decreasing = TRUE)
# matching positive words
neg.matches = match(names(freq), neg_words)
neg.matches = !is.na(neg.matches)
freq_neg <- freq[neg.matches]
names <- names(freq_neg)
windows()
wordcloud(names,freq_neg,scale = c(4,0.5), min.freq = 1, colors = brewer.pal(8,"Dark2"))
}
negwordc(data_tdm)
#install.packages("rvest")  ## for web scripting
library(rvest)
#install.packages("XML")  ## we know that web are in html format so
library(XML)
#install.packages("magrittr") ### increase performance
library(magrittr)
library("tm")
library("slam")
library(topicmodels)
library(NLP)
essay_assesment <- readLines("C:/Users/Sanchi/Desktop/Assignment/Complete/R/NLP/essay.txt")
essay_assesment
length(essay_assesment)
## corpus of data
review<- Corpus(VectorSource(essay_assesment))
review
######cleaning
review <- tm_map(review,toupper)
inspect(review[10])
review <- tm_map(review,removePunctuation)
inspect(review[10])
review <- tm_map(review,stripWhitespace)
inspect(review[10])
review <- tm_map(review,removeNumbers)
inspect(review[10])
review <- tm_map(review,removeWords,stopwords('english'))
inspect(review[10])
stopword <- readLines("C:/Users/Sanchi/Desktop/Assignment/Complete/R/NLP/stop.txt")
review <- tm_map(review,removeWords,stopword)
inspect(review[12])
# Remove URL's from corpus
removeURL <- function(z) gsub('http[[:alnum:]]*', '', z)
review<- tm_map(review, content_transformer(removeURL))
inspect(review[14])
#####EDA
## tdm or dtm
dtm <- DocumentTermMatrix(review)
dim(dtm)
dtm$nrow
dtm$ncol
dtm$dimnames
dtm$i
dtm$v
dtm$j
dtm1 <- as.matrix(dtm)
## bag oe word
ro <- apply(dtm1,1,sum)
ro
dtmnew <-dtm1[ro>0,]
View(dtmnew)
lda <- LDA(dtmnew,5)
terms <- terms(lda,3)
terms
tops <- terms(lda)
tops
ta <- table(names(tops),unlist(tops))
ta
ta <- data.frame(ta)
ta
############clustering
cls <- hclust(dist(ta),method = "ward.D2")
par(family="HiraKakuProN-W3")
plot(cls)
##### emotion Mining
library(syuzhet)
myfile <- readLines("C:/Users/Sanchi/Desktop/Assignment/Complete/R/NLP/essay.txt")
sentence <- get_sentences(myfile)
class(sentence)
sentiment <- get_sentiment(sentence,method = "bing")
nrc_vector <- get_sentiment(sentence,method = "nrc")
head(nrc_vector)
class(sentiment)
plot(sentiment)
sum(sentiment)
mean(sentiment)
head(sentiment)
head(sentence)
summary(sentiment)
plot(sentiment,type = "l",main = "plot Trajectory",
xlab = "Narrative Time",ylab = "Emotion Valence")
abline(h=0,col="Green")
###extract positive and negative emotion
neg <- sentence[which.min(sentiment)]
neg
pos <- sentence[which.max(sentiment)]
pos
### in  more depth
poa_v <- myfile
poa_sent <- get_sentiment(poa_v, method = "bing")
plot(poa_sent,type = "h", main = "LOTR using transformed Values",
xlab = "Narrative Time", ylab = "Emotinal Valence",col="red")
ft_values <- get_transformed_values(poa_sent,
low_pass_size = 3,
x_reverse_len = 100,
scale_vals = TRUE,
scale_range = FALSE)
plot(ft_values, type = "h", main = "LOTR using Transformed values",
xlab = "Narrative time", ylab = "Emotional Valence",
col="red")
nrc_data <- get_nrc_sentiment(sentence)
nrc_score_sent <- get_nrc_sentiment(negative)
nrc_score_word <- get_nrc_sentiment('grim')
#subset
sad_items <- which(nrc_data$sadness>0)
head(sentence[sad_items])
barplot(sort(colSums(prop.table(nrc_data[,1:8]))), horiz = T, cex.names = 0.7,
las= 1, main = "word concurrency", xlab="Percentage",col = 1:8)
######  trust is maximum in the review
## disgust is min or least according to review
## we can say that maximum number of coustmer are satisfied with product
## so we can say that product is good
#install.packages("rvest")  ## for web scripting
library(rvest)
#install.packages("XML")  ## we know that web are in html format so
library(XML)
#install.packages("magrittr") ### increase performance
library(magrittr)
library("tm")
library("slam")
library(topicmodels)
library(NLP)
essay_assesment <- readLines("C:/Users/Sanchi/Desktop/Assignment/Complete/R/NLP/essay.txt")
essay_assesment
length(essay_assesment)
## corpus of data
review<- Corpus(VectorSource(essay_assesment))
review
######cleaning
review <- tm_map(review,toupper)
inspect(review[10])
review <- tm_map(review,removePunctuation)
inspect(review[10])
review <- tm_map(review,stripWhitespace)
inspect(review[10])
review <- tm_map(review,removeNumbers)
inspect(review[10])
review <- tm_map(review,removeWords,stopwords('english'))
inspect(review[10])
stopword <- readLines("C:/Users/Sanchi/Desktop/Assignment/Complete/R/NLP/stop.txt")
review <- tm_map(review,removeWords,stopword)
inspect(review[12])
# Remove URL's from corpus
removeURL <- function(z) gsub('http[[:alnum:]]*', '', z)
review<- tm_map(review, content_transformer(removeURL))
inspect(review[14])
#####EDA
## tdm or dtm
dtm <- DocumentTermMatrix(review)
dim(dtm)
dtm$nrow
dtm$ncol
dtm$dimnames
dtm$i
dtm$v
dtm$j
dtm1 <- as.matrix(dtm)
## bag oe word
ro <- apply(dtm1,1,sum)
ro
dtmnew <-dtm1[ro>0,]
View(dtmnew)
lda <- LDA(dtmnew,5)
terms <- terms(lda,3)
terms
tops <- terms(lda)
tops
ta <- table(names(tops),unlist(tops))
ta
ta <- data.frame(ta)
ta
############clustering
cls <- hclust(dist(ta),method = "ward.D2")
par(family="HiraKakuProN-W3")
plot(cls)
##### emotion Mining
library(syuzhet)
myfile <- readLines("C:/Users/Sanchi/Desktop/Assignment/Complete/R/NLP/essay.txt")
sentence <- get_sentences(myfile)
class(sentence)
sentiment <- get_sentiment(sentence,method = "bing")
nrc_vector <- get_sentiment(sentence,method = "nrc")
head(nrc_vector)
class(sentiment)
plot(sentiment)
sum(sentiment)
mean(sentiment)
head(sentiment)
head(sentence)
summary(sentiment)
plot(sentiment,type = "l",main = "plot Trajectory",
xlab = "Narrative Time",ylab = "Emotion Valence")
abline(h=0,col="Green")
###extract positive and negative emotion
neg <- sentence[which.min(sentiment)]
neg
pos <- sentence[which.max(sentiment)]
pos
### in  more depth
poa_v <- myfile
poa_sent <- get_sentiment(poa_v, method = "bing")
plot(poa_sent,type = "h", main = "LOTR using transformed Values",
xlab = "Narrative Time", ylab = "Emotinal Valence",col="red")
ft_values <- get_transformed_values(poa_sent,
low_pass_size = 3,
x_reverse_len = 100,
scale_vals = TRUE,
scale_range = FALSE)
plot(ft_values, type = "h", main = "LOTR using Transformed values",
xlab = "Narrative time", ylab = "Emotional Valence",
col="red")
nrc_data <- get_nrc_sentiment(sentence)
nrc_score_sent <- get_nrc_sentiment(negative)
nrc_score_word <- get_nrc_sentiment('grim')
#subset
sad_items <- which(nrc_data$sadness>0)
head(sentence[sad_items])
barplot(sort(colSums(prop.table(nrc_data[,1:8]))), horiz = T, cex.names = 0.7,
las= 1, main = "word concurrency", xlab="Percentage",col = 1:8)
######  trust is maximum in the review
## disgust is min or least according to review
## we can say that maximum number of coustmer are satisfied with product
## so we can say that product is good
library(Amelia)
library(psych)
library(lubridate)
library(VIM)
library(e1071)
library(mice)
library(Hmisc)
library(data.table)
library(dplyr)
library(car)
#library(ggplot2)
#library(stringr)
#library(DT)
#library(tidyr)
#library(knitr)
#library(rmarkdown)
sales <- read.csv("F://Akarshan's Document//Akarshan//excelrDATASCIENCE//Project1//Data Sets//finalsales.csv")
########EDA#########
## remove x
sales <- sales[,c(-1)]
str(sales)
## change the format of date
sales$Date <-mdy(sales$Date)
## change the clm name
setnames(sales,c("Dealer.ID"),c("dealer"))
#sales$Date <- as.Date(sales$Date,format="%d/%m/%Y")
#order(sales$Date)
sum(is.na(sales$Date))
sum(is.na(sales))
table(is.na(sales))
colSums(is.na(sales))
## 5 missing valiue in value
which(is.na(sales$Value))
#1138 2475 2476 2477 3675 these row contain NA in value data
## all the BM
#business moment
describe.by(sales)
## here we get the mean median sd range skew kurtosisi
summary(sales)
## so there is to  much diffrence in mean and median so there is a possibility of outlier
### check for the outlier
boxplot(sales$Quantity, main="Quantity with outlier")
outlier_values<- as.data.frame(boxplot.stats(sales$Quantity)$out)  # outlier values.
hist(sales$Quantity)
## pattern of missing value
md.pattern(sales)
windows()
##visualization odf missing value
misisng_pttern <- aggr(sales, col=c('navyblue','yellow'),
numbers=TRUE, sortVars=TRUE,
labels=names(sales), cex.axis=.7,
gap=3, ylab=c("Missing data","Pattern"))
##impute missing value
#impute_value <- mice(sales,m=5,method="pmm",maxit = 5)
##this is a unbalanced data
library(Hmisc)
sales$Value<- impute(sales$Value,fun = mean)
sum(is.imputed(sales$Value))
colSums(is.na(sales))
describe(sales$Value)
summary(sales$Value)
colSums(is.na(sales))
## so there is no na value
library(tidyr)
library(knitr)
library(rmarkdown)
library(dplyr)
### RFM
###Recency, Frequency, & Monetary (RFM)
##We first need to calculate the max transaction date to calculate the recency score
max_Date <- max(sales$Date)
## now i am going to calculate the Recency
Recency <- sales %>%
group_by(dealer) %>%
summarise(recency=as.numeric(as.Date("2018-03-30")-max(Date)))
View(Recency)
summary(Recency)
library(plyr)
frequency <- ddply(sales,.(dealer,Date),summarize,sum_freq=sum(Value),frequency=length(dealer))
View(frequency)
str(frequency)
Frequency <- ddply(frequency,.(dealer),summarize,frequency=length(dealer))
View(Frequency)
###Monetary #######
#Sum the amount of money a customer spent and divide it by Frequency,
#to get the amount per transaction on average, that is the Monetary data.
sum_value <- ddply(sales,.(dealer),summarize,sum_value=sum(Value))
View(sum_value)
## frequency with sum value
sales_F_M<- merge(Frequency, sum_value, by="dealer")
View(sales_F_M)
monetary <- sales_F_M$sum_value/sales_F_M$frequency
sales_F_M$monetary <-sales_F_M$sum_value / sales_F_M$frequency
View(sales_F_M)
#We have calculated monetary.
#we remove the sum_value column
sales_F_M <- sales_F_M[,-3]
View(sales_F_M)
# We interested in Recency, Frequency and Monetary###
rfm_result <- cbind(Recency,sales_F_M)
View(rfm_result)
rfm_result <- rfm_result[,-c(3)]
View(rfm_result)
#setwd("F://Akarshan's Document//Akarshan//excelrDATASCIENCE//Project1//Data Sets")
#getwd()
#write.csv(rfm_result,file = "rfm.csv")
##graphical presentation
hist(rfm_result$recency)
hist(rfm_result$frequency)
hist(rfm_result$monetary)
retrunrfm <- read.csv("F://Akarshan's Document//Akarshan//excelrDATASCIENCE//Project1//Data Sets//returnsrfm.csv")
salesrfm <- read.csv("F://Akarshan's Document//Akarshan//excelrDATASCIENCE//Project1//Data Sets//rfm.csv")
#a=merge(retrunrfm,salesrfm)
library(data.table)
setnames(retrunrfm,"Dealer.ID","dealer")
library(dplyr)
full <- full_join(salesrfm,retrunrfm,by=NULL)
colSums(is.na(full))
View(full)
View(full)
View(full)
View(full)
View(full)
